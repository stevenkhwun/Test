---
title: "Exploratory Data Analysis"
author: "SW"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true # table of content true
    toc_float: true
    toc_depth: 3  # upto three depths of headings (specified by #, ## and ###)
    number_sections: true  ## if you want number sections at each table header
    theme: cerulean  # many options for theme, this one is my favorite.
    highlight: tango  # specifies the syntax highlighting style
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Estimates of Location

## The data


```{r}
# Get the data
csv <- "https://raw.githubusercontent.com/stevenkhwun/Online-resources/refs/heads/main/Data/Practical-Statistics/state.csv"
state <- read.csv(csv)
# View the dataset
dim(state)
head(state, n = 5)
summary(state)
```

## Mean, trimmed mean, and median

### Mean (average)
The sum of all values divided by the number of values.

```{r}
# Mean
mean(state[["Population"]])
```

### Trimmed mean (truncated mean)
The average of all values after dropping a fixed number of extreme values. It is calculated by dropping a fixed number of sorted values at each end and then taking an average of the remaining values.

A trimmed mean eliminates the influence of extreme values.

```{r}
# Trimmed mean
# 'trim=0.1' drops 10% from each end
mean(state[["Population"]], trim = 0.1)
```

A further example for trimmed mean:
```{r}
# Data with all the middle values equal to 4
skewdata <- c(1,4,4,4,4,4,4,4,4,1000)
```

```{r}
# The mean is influenced by the outlier 1000
mean(skewdata)
# The trimmed mean is calculated by dropping the first and last observation
mean(skewdata, trim=0.1)
```

### Median (50th percentile)
The value such that one-half of the data lies above and below.

Compared to the mean, which uses all observations, the median depends only on the values in the center of the sorted data. While this might seem to be a disadvantage, since the mean is much more sensitive to the data, there are many instances in which the median is a better metric for location.


```{r}
# Median
median(state[["Population"]])
```

## Weighted mean and weighted median

### Weighted mean (weighted average)
The sum of all values times a weight divided by the sum of the weights.

There are two main motivations for using a weighted mean:

*   Some values are intrinsically more variable than others, and highly variable observations are given a lower weight.
*   The data collected does not equally represent the different group that we are interested in mearsuring.

If we want to compute the average murder rate for the country, we need to use a weighted mean or median to account for different populations in the states.


```{r}
# Weighted mean
weighted.mean(state[["Murder.Rate"]], w=state[["Population"]])
```

### Weighted median
The value such that one-half of the sum of the weights lies above and below the sorted data.


> Notes on _R_:<br>
  Since base _R_ doesn't have a function for weighted median, We need to install a package such as `matrixStats`.

```{r}
library("matrixStats")
weightedMedian(state[["Murder.Rate"]], w=state[["Population"]])
```

## Outliers
The median is referred to as a robust estimate of location since it is not influenced by outliers (extreme cases) that could skew the results. An outlier is any value that is very distant from the other values in a data set. Being an outlier in itself does not make a data value invalid or erroneous. When outliers are the result of bad data, the mean will result in a poor estimate of location, while the median will still be valid.

The median is not the only robust estimate of location. In fact, a trimmed mean is widely used to avoid the influence of outliers. The trimmed mean can be thought of as a compromise between the median and the mean: it is robust to extreme values in the data, but uses more data to calculate the estimate for location.

_Robust_, or resistant, means not sensitive to extreme values.

# Estimates of Variability

## Key Terms for Variability Metrics

* Deviations (errors, residuals): The difference between the observed values and the estimate of location.
* Variance (mean-squared-error): The sum of squared deviations from the mean divided by $n-1$ where $n$ is the number of data values.
* Standard deviation: The square root of the variance.
* Mean absolute deviation (l1-norm, Manhattan norm): The mean of the absolute values of the deviations from the mean.
* Median absolute deviation from the median (MAD): The median of the absolute values of the deviations from the median.
* Range: The difference between the largest and the smallest value in a data set.
* Order statistics (ranks): Metrics based on the data values sorted from smallest to biggest.
* Percentile (quantile): The value such that $P$ percent of the values take on this value or less and $(100-P)$ percent take on this value or more.
* Interquartile range (IQR): The difference between 75th percentile and the 25th percentile.

Neither the variance, the standard deviation, nor the mean absolute deviation is robust to outliers and extreme values. The variance and standard deviation are especially sensitive to outliers since they are based on the squared deviations. A robust estimate of variability is the _median absolute deviation from the median_ or __MAD__.

## Using _R_'s built-in functions for the standard deviation, the interquartile range (IQR), and the median absolute deviation from the median (MAD)

```{r}
# Standard deviation
sd(state[["Population"]])
# Interquartile range
IQR(state[["Population"]])
# Median absolute deviation from the median
mad(state[["Population"]])
```

The standard deviation is almost twice as large as the __MAD__. This is not surprising since the standard deviation is sensitive to outliers.


# Exploring the Data Distribution

## Key Terms for Exploring the Distribution

* Boxplot (box and whiskers plot): A plot introduced by Tukey as a quick way to visualize the distribution of data.
* Frequency table: A tally of the count of numeric data values that fall into a set of intervals (bins).
* Histogram: A plot of the frequency table with the bins on the x-axis and the count (or proportion) on the y-axis.
* Density plot: A smoothed version of the histogram, often based on a _kernel density estimate_.

## Percentiles and Boxplots

```{r}
# Percentiles
quantile(state[["Murder.Rate"]], p = c(.05, .25, .5, .75, .95))
# Boxplot
boxplot(state[["Population"]]/1000000, ylab = "Population (millions)")
```

## Frequency Tables and Histograms

```{r}
# Frequency table
breaks <- seq(from = min(state[["Population"]]),
              to = max(state[["Population"]]), length = 11)
pop_freq <- cut(state[["Population"]], breaks = breaks,
                right = TRUE, include.lowest = TRUE)
table(pop_freq)
# Histogram
options(scipen = 5)
hist(state[["Population"]], breaks = breaks, main = "Histogram of state population",
     xlab = "Population")
```

> Notes on _R_:<br>
  The option `options(scipen = 5)` determines how likely _R_ is to switch to scientific notation in the plot. The higher the number, the less likely _R_ will switch to scientific notation.

## Statistical Moments

In statistical theory, location and variability are referred to as the first and second _moments_ of a distribution. The third and fouth moments are called _skewness_ and _kurtosis_. __Skewness__ refers to whether the data is skewed to larger or smaller values, and __kurtosis__ indicates the propensity of the data to have extreme values. Generally, metrics are not used to measure skewness and kurtosis; instead, these are discovered through visual displays.

## Density Plots and Estimates

Related to the histogram is a density plot, which shows the distribution of data values as a continuous line. A density plot can be thought of as a smoothed histogram, although it is typically computed directly from the data through a _kernel density estimate_. In _R_, you can compute a density estimate using the `density` function:

```{r}
hist(state[["Murder.Rate"]], freq = FALSE, main = "State Murder Rate",
          xlab = "Murder rate (murders per 100,000 people)")
lines(density(state[["Murder.Rate"]]), lwd = 3, col = "blue")
```

# Exploring Binary and Categorical Data

## The data

```{r}
# Get the data
csv <- "https://raw.githubusercontent.com/stevenkhwun/Online-resources/refs/heads/main/Data/Practical-Statistics/dfw_airline.csv"
dfw <- read.csv(csv)
dfw
```

## Bar Chart

```{r}
# Bar chart
barplot(as.matrix(dfw) / 6, cex.axis = 0.8, cex.names = 0.7,
        xlab = "Cause of delay", ylab = "Count")
```

Note that a bar chart resembles a histogram; in a bar chart the x-axis represents different categories of a factor variable, while in a histogram the x-axis represents values of a single variable on a numeric scale. In a histogram, the bars are typically shown touching each other, with gaps indicating values that did not occur in the data. In a bar chart, the bars are shown separae from one another.

# Correlation

## Key Terms for Correlation

* Correlation coefficient: A metric that measures the extent to which numeric variables are associated with one another (ranges from -1 to +1)
* Correlation matrix: A table where the variables are shown on both rows and columns, and the cell values are the correlations between the variables.
* Scatterplot: A plot in which the x-axis is the value of one variable, and the y-axis the value of another.

## The data

__`sp500_px`__ data

```{r}
# Access the data from my Dell Laptop
data_path <- "C:/Users/steve/GitHub/Online-resources/Data/Practical-Statistics"
sp500_px <- read.csv(file.path(data_path, "sp500_data.csv.gz"), row.names = 1)
dim(sp500_px)
```

__`sp500_sym`__ data

```{r}
# Access the data from GitHub
csv <- "https://raw.githubusercontent.com/stevenkhwun/Online-resources/refs/heads/main/Data/Practical-Statistics/sp500_sectors.csv"
sp500_sym <- read.csv(csv, stringsAsFactors = FALSE)
dim(sp500_sym)
```

## Correlation matrix

```{r}
# Telecommunication stocks data
telecom <- sp500_px[, sp500_sym[sp500_sym$sector == 'telecommunications_services', 'symbol']]
head(telecom, n = 5)
# Create the correlation matrix
telecom <- telecom[row.names(telecom) > '2012-07-01',]
telecom_cor <- cor(telecom)
telecom_cor
```

> Notes on _R_:<br>
  In _R_, we can easily create a table of correlations using the package `corrplot`.


```{r message=FALSE}
# Load the library
library(corrplot)
```

The following gigure shows the correlation between the daily returns for major exchange-traded funds (ETFs).

```{r}
# Create the data
etfs <- sp500_px[row.names(sp500_px) > '2012-07-01',
                 sp500_sym[sp500_sym$sector == 'etf', 'symbol']]
# Create the plot
corrplot(cor(etfs), method = 'ellipse')
```

## Scatterplots

The standard way to visualize the relationship between two measured data variables is with a scatterplot. The x-axis represents one variable and the y-axis another, and each point on the graph is a record.

```{r}
# Scatterplot
plot(telecom$T, telecom$VZ, xlab = "ATT (T)", ylab = "Verizon (VZ)")
```

