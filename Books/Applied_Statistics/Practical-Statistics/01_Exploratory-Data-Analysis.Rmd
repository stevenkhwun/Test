---
title: "Exploratory Data Analysis"
author: "SW"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Estimates of Location

### The data


```{r}
# Get the data
csv <- "https://raw.githubusercontent.com/stevenkhwun/Online-resources/refs/heads/main/Data/Practical-Statistics/state.csv"
state <- read.csv(csv)
# View the dataset
dim(state)
head(state, n = 5)
summary(state)
```

### Mean, trimmed mean, and median

Compute the mean, trimmed mean, and median for the population using _R_:

```{r}
# Mean
mean(state[["Population"]])
# Trimmed mean
mean(state[["Population"]], trim = 0.1)
# Median
median(state[["Population"]])
```

### Weighted mean and weighted median

```{r}
# Weighted mean
weighted.mean(state[["Murder.Rate"]], w=state[["Population"]])
```

> Notes on _R_:<br>
  Since base _R_ doesn't have a function for weighted median, We need to install a package such as `matrixStats`.

```{r}
library("matrixStats")
weightedMedian(state[["Murder.Rate"]], w=state[["Population"]])
```


## Estimates of Variability

### Key Terms for Variability Metrics

* Deviations (errors, residuals): The difference between the observed values and the estimate of location.
* Variance (mean-squared-error): The sum of squared deviations from the mean divided by $n-1$ where $n$ is the number of data values.
* Standard deviation: The square root of the variance.
* Mean absolute deviation (l1-norm, Manhattan norm): The mean of the absolute values of the deviations from the mean.
* Median absolute deviation from the median (MAD): The median of the absolute values of the deviations from the median.
* Range: The difference between the largest and the smallest value in a data set.
* Order statistics (ranks): Metrics based on the data values sorted from smallest to biggest.
* Percentile (quantile): The value such that $P$ percent of the values take on this value or less and $(100-P)$ percent take on this value or more.
* Interquartile range (IQR): The difference between 75th percentile and the 25th percentile.

Neither the variance, the standard deviation, nor the mean absolute deviation is robust to outliers and extreme values. The variance and standard deviation are especially sensitive to outliers since they are based on the squared deviations. A robust estimate of variability is the _median absolute deviation from the median_ or __MAD__.

### Using _R_'s built-in functions for the standard deviation, the interquartile range (IQR), and the median absolute deviation from the median (MAD)

```{r}
# Standard deviation
sd(state[["Population"]])
# Interquartile range
IQR(state[["Population"]])
# Median absolute deviation from the median
mad(state[["Population"]])
```

The standard deviation is almost twice as large as the __MAD__. This is not surprising since the standard deviation is sensitive to outliers.


## Exploring the Data Distribution

### Key Terms for Exploring the Distribution

* Boxplot (box and whiskers plot): A plot introduced by Tukey as a quick way to visualize the distribution of data.
* Frequency table: A tally of the count of numeric data values that fall into a set of intervals (bins).
* Histogram: A plot of the frequency table with the bins on the x-axis and the count (or proportion) on the y-axis.
* Density plot: A smoothed version of the histogram, often based on a _kernel density estimate_.

### Percentiles and Boxplots

```{r}
# Percentiles
quantile(state[["Murder.Rate"]], p = c(.05, .25, .5, .75, .95))
# Boxplot
boxplot(state[["Population"]]/1000000, ylab = "Population (millions)")
```

### Frequency Tables and Histograms

```{r}
# Frequency table
breaks <- seq(from = min(state[["Population"]]),
              to = max(state[["Population"]]), length = 11)
pop_freq <- cut(state[["Population"]], breaks = breaks,
                right = TRUE, include.lowest = TRUE)
table(pop_freq)
# Histogram
options(scipen = 5)
hist(state[["Population"]], breaks = breaks, main = "Histogram of state population",
     xlab = "Population")
```

> Notes on _R_:<br>
  The option `options(scipen = 5)` determines how likely _R_ is to switch to scientific notation in the plot. The higher the number, the less likely _R_ will switch to scientific notation.

### Statistical Moments

In statistical theory, location and variability are referred to as the first and second _moments_ of a distribution. The third and fouth moments are called _skewness_ and _kurtosis_. __Skewness__ refers to whether the data is skewed to larger or smaller values, and __kurtosis__ indicates the propensity of the data to have extreme values. Generally, metrics are not used to measure skewness and kurtosis; instead, these are discovered through visual displays.

### Density Plots and Estimates

Related to the histogram is a density plot, which shows the distribution of data values as a continuous line. A density plot can be thought of as a smoothed histogram, although it is typically computed directly from the data through a _kernel density estimate_. In _R_, you can compute a density estimate using the `density` function:

```{r}
hist(state[["Murder.Rate"]], freq = FALSE, main = "State Murder Rate",
          xlab = "Murder rate (murders per 100,000 people)")
lines(density(state[["Murder.Rate"]]), lwd = 3, col = "blue")
```

## Exploring Binary and Categorical Data

### The data

```{r}
# Get the data
csv <- "https://raw.githubusercontent.com/stevenkhwun/Online-resources/refs/heads/main/Data/Practical-Statistics/dfw_airline.csv"
dfw <- read.csv(csv)
dfw
```

### Bar Chart

```{r}
# Bar chart
barplot(as.matrix(dfw) / 6, cex.axis = 0.8, cex.names = 0.7,
        xlab = "Cause of delay", ylab = "Count")
```

Note that a bar chart resembles a histogram; in a bar chart the x-axis represents different categories of a factor variable, while in a histogram the x-axis represents values of a single variable on a numeric scale. In a histogram, the bars are typically shown touching each other, with gaps indicating values that did not occur in the data. In a bar chart, the bars are shown separae from one another.

## Correlation

```{r}
# Access the data from my Dell Laptop
data_path <- "C:/Users/steve/GitHub/Online-resources/Data/Practical-Statistics"
sp500_px <- read.csv(file.path(data_path, "sp500_data.csv.gz"), row.names = 1)
dim(sp500_px)
```




```{r}
# Access the data from GitHub
csv <- "https://raw.githubusercontent.com/stevenkhwun/Online-resources/refs/heads/main/Data/Practical-Statistics/sp500_sectors.csv"
sp500_sym <- read.csv(csv, stringsAsFactors = FALSE)
dim(sp500_sym)
```


```{r}
etfs <- sp500_px[row.names(sp500_px) > '2012-07-01',
                 sp500_sym[sp500_sym$sector == 'etf', 'symbol']]
library(corrplot)
corrplot(cor(etfs), method = 'ellipse')
```

```{r}
telecom <- sp500_px[, sp500_sym[sp500_sym$sector == 'telecommunications_services', 'symbol']]
dim(telecom)
head(telecom, n = 5)
telecom <- telecom[row.names(telecom) > '2012-07-01',]
telecom_cor <- cor(telecom)
telecom_cor
```